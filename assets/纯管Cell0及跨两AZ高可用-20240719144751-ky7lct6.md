<!-- TOC -->
* [需求和背景描述](#需求和背景描述)
* [纯管Cell0](#纯管cell0)
  * [纯管Cell0标识](#纯管cell0标识)
  * [避免业务实例调度到纯管Cell0](#避免业务实例调度到纯管cell0)
  * [纯管Cell0任意门使用改造](#纯管cell0任意门使用改造)
    * [Egress场景](#egress场景)
    * [Ingress场景](#ingress场景)
      * [适合管控服务访问管控自己在业务网内实例](#适合管控服务访问管控自己在业务网内实例)
      * [适合管控服务访问租户在业务网内实例](#适合管控服务访问租户在业务网内实例)
      * [业务自己实现了cell agent的场景](#业务自己实现了cell-agent的场景)
    * [附录：管控服务在业务网内部署高可用实例（非租户实例）](#附录管控服务在业务网内部署高可用实例非租户实例)
  * [纯管cell0场景下任意门方案](#纯管cell0场景下任意门方案)
* [Cell0跨两AZ高可用](#cell0跨两az高可用)
  * [整体方案](#整体方案)
    * [增加第三方仲裁节点](#增加第三方仲裁节点)
    * [支持zone级别的拓扑管理](#支持zone级别的拓扑管理)
  * [zone级别的拓扑管理](#zone级别的拓扑管理)
    * [节点增加zone属性](#节点增加zone属性)
      * [全新安装部署](#全新安装部署)
      * [扩容节点](#扩容节点)
      * [现有环境升级](#现有环境升级)
    * [将多个副本分散到不同的AZ](#将多个副本分散到不同的az)
      * [pod配置zone级别的pod反亲和](#pod配置zone级别的pod反亲和)
      * [pod配置拓扑分布约束topologySpreadConstraints](#pod配置拓扑分布约束topologyspreadconstraints)
      * [全局配置默认AZ级别的打散](#全局配置默认az级别的打散)
    * [通过descheduler再平衡pod实现跨zone打散](#通过descheduler再平衡pod实现跨zone打散)
    * [zone级别的亲和](#zone级别的亲和)
  * [运行仲裁服务](#运行仲裁服务)
    * [仲裁节点初始化](#仲裁节点初始化)
      * [为仲裁服务设置污点容忍](#为仲裁服务设置污点容忍)
      * [非关键组件不要调度到仲裁节点上](#非关键组件不要调度到仲裁节点上)
        * [措施1：计算副本数时排除掉仲裁节点](#措施1计算副本数时排除掉仲裁节点)
        * [措施2：设置仲裁节点反亲和](#措施2设置仲裁节点反亲和)
    * [仲裁节点软硬件要求](#仲裁节点软硬件要求)
    * [网络通信](#网络通信)
  * [故障切换和高可用详细设计](#故障切换和高可用详细设计)
    * [etcd](#etcd)
      * [高可用指标](#高可用指标)
      * [高可用部署架构](#高可用部署架构)
        * [容灾场景1：etcd leader所在节点的AZ发生故障](#容灾场景1etcd-leader所在节点的az发生故障)
        * [容灾场景2：非etcd leader所在节点的AZ发生故障](#容灾场景2非etcd-leader所在节点的az发生故障)
        * [容灾场景3：AZ间网络隔离](#容灾场景3az间网络隔离)
      * [详细设计](#详细设计)
        * [安装etcd仲裁服务](#安装etcd仲裁服务)
        * [管控节点etcd标签管理](#管控节点etcd标签管理)
        * [etcd节点标签控制器](#etcd节点标签控制器)
    * [mysql](#mysql)
    * [minio](#minio)
    * [ovsdb](#ovsdb)
    * [redis](#redis)
      * [当前拉新从条件](#当前拉新从条件)
      * [跨 AZ 高可用](#跨-az-高可用)
    * [kafka](#kafka)
    * [clickhouse](#clickhouse)
    * [dts](#dts)
  * [部署和扩容](#部署和扩容)
    * [两AZ高可用部署](#两az高可用部署)
    * [Region内AZ扩容](#region内az扩容)
  * [友商参考对比](#友商参考对比)
    * [华为云](#华为云)
      * [CECSTACK V5和华为云跨两个AZ高可用方案对比](#cecstack-v5和华为云跨两个az高可用方案对比)
<!-- TOC -->

# 需求和背景描述
在 [CECSTACK V5多Region多AZ部署最佳实践](resources/CECSTACK多Region多AZ最佳实践V1.3.pptx) 中对各Region内管理集群**Cell0跨AZ高可用**提出要求。
更进一步，为简化Cell0跨AZ高可用实现（及Region容灾备份），跨AZ高可用部署的Cell0不带业务节点/Pool，即以**纯管控模式部署的Cell0**。

<img src="images/region-az-arch.png" alt="region-az-arch" width="60%" />

* 纯管Cell0没有计算、NFV等节点，不支持业务网、sysvpc，管控创建的任意门及其使用需要调整。
* Cell0跨AZ高可用，关键的有状态服务有etcd、mysql、minio、ovsdb、redis、kafka、clickhouse、dts，其设计和实现难点在于两AZ时如何确保选主和分布式一致性。<br>
  <img src="images/controlplane-az-ha.png" alt="control-plane-az-ha" width="60%" />

# 纯管Cell0
## 纯管Cell0标识
**注意**，所有Cell0集群都为Karmada自纳管集群（hubplusmember角色）。
hubplusmember角色为Karmada多集群管理内部实现所用，同CECSTACK V5 Cell0是否为纯管集群无关、不是一个概念。
> hubplusmember和member是karmada cluster role - 这是karmada上集群的终态。
> hub是中间态（表现刚拉起，尚未被karmada管理）。

为此，在安装部署时取消`集群自纳管`选项（所有控制集群都是Karmada自纳管集群），增加`是否纯管理集群`的选项：

<img src="images/pure-control-cluster.png" alt="pure-control-cluster" width="40%" />

当选择纯管集群后，安装部署时集群不能选取、配置业务节点。
查询接口方面，`kube-system` 命名空间下 `cm/cluster-config-v1` 新增`bool`字段 `isPureControlCluster: true` 表明为纯管集群。

## 避免业务实例调度到纯管Cell0
纯管Cell0不带业务节点，要避免业务实例调度上去。
多集群管理服务控制器自动将纯管Cell0打污点`pure-control-cluster:NoSchedule`，此后Karmada默认不往Cell0集群上调度资源对象。
需要往Cell0集群调度的工作负载/实例，应主动容忍该污点。

需要补充说明的：
> 针对一个AZ下某个CellX业务集群没有业务节点，但云服务实例还是被调度上去的问题，需要该云服务像ECS那样提供webhook，实现精细调度。
> 现支持业务用一个CR来实现lua  webhook，文档和示例详见链接 https://karmada.io/docs/userguide/globalview/customizing-resource-interpreter/#write-with-configuration

根据是否为纯管集群的标识isPureControlCluster，不在纯管Cell0上拉起经典vpc，相应的管控创建的任意门及其使用需要调整。

## 纯管Cell0任意门使用改造
胶片链接[纯管cell0 - 任意门使用改造](https://wiki.cestc.cn/pages/viewpage.action?pageId=367498841)。

后文在说明流量方向时，站在业务网角度：
* *Egress*，表示从`业务网`向`管理网`的流量
* *Ingress*，表示从`管理网`向`业务网`的流量

### Egress场景
![img.png](images/egress-flow.png)

### Ingress场景
#### 适合管控服务访问管控自己在业务网内实例
![img.png](images/ingress-management-in-vpc.png)

#### 适合管控服务访问租户在业务网内实例
![img.png](images/ingress-tenant-in-vpc.png)

#### 业务自己实现了cell agent的场景
![img.png](images/ingress-self-cell-agent.png)

### 附录：管控服务在业务网内部署高可用实例（非租户实例）
![img.png](images/management-ha-in-vpc.png)

## 纯管cell0场景下任意门方案
方案详见wiki链接[纯管cell0场景下任意门方案](https://wiki.cestc.cn/pages/viewpage.action?pageId=364826003) 。

高可用任意门网关创建示例请见wiki链接[高可用任意门网关](https://wiki.cestc.cn/pages/viewpage.action?pageId=288243170) 。

# Cell0跨两AZ高可用
## 整体方案
### 增加第三方仲裁节点
CECSTACK V5的跨两个AZ高可用方案，采用 **2+2+第三方仲裁节点** 的方式：

![img.png](images/通过仲裁节点解决两个AZ管理集群高可用部署.png)

> 注意：
> 仲裁节点属于管理集群，但只调度、运行关键的有状态服务，例如etcd、mysql、minio、ovsdb等，
> 其余pod均不能运行在仲裁节点上。

仲裁节点上运行的仲裁服务包括：
1. ccos `etcd`仲裁服务
2. ccos `ovsdb-server`仲裁服务
3. ccos kafka的`zookeeper`仲裁服务
4. ccos clickhouse的`zookeeper`仲裁服务
5. ccos dts的`zookeeper`仲裁服务
6. ccos `minio`仲裁服务

其它有状态服务，其Operator/控制器通过`k8s leaderelection`库，基于lease资源完成主从选举，做故障切换和高可用。
基于lease资源的主从选举最终由etcd集群保证仲裁结果的一致性。

除上述方案外，各有状态服务也可通过修改高可用模式，最终实现管理集群跨两个AZ的高可用部署。

### 支持zone级别的拓扑管理
管理集群cell0跨AZ部署，需要为集群节点（特别是管控节点）增加zone标签，作为拓扑管理依据，以实现：
* 无状态应用，**将多个副本分散到不同的AZ中**，每个AZ/节点负载分担
* 有状态应用，**将多个副本分散到不同的AZ中**，跨AZ高可用部署，并能同仲裁服务对接
* 另一方面，存在依赖关系、有大量访问流量的一组pod，将它们**亲和到同一AZ中**，能够缩短访问延迟，同时尽量降低跨AZ间网络流量

为集群节点增加zone标签的示意图如下：<br>
![img.png](images/cell0-zone-labeled.png)

## zone级别的拓扑管理

### 节点增加zone属性
**安装部署**和**扩容时**，支持用户如实填写节点所属可用区zone信息。

> 仲裁节点是管理集群中一个特殊节点，有如下约束和注意项：
> 1. 一个集群至多只有一个仲裁节点
> 2. 仲裁节点位于特殊的“仲裁”区域，其zone标签约定为 *zone.node-label.ccos.io=reserved-arbitration*

具体的，为节点增加zone属性，然后渲染生成一个`nodelabel/nodes-zone-label`资源文件，为各节点配置zone标签，例如：
```yaml
apiVersion: nlo.ccos.io/v1beta1
kind: NodeLabel
metadata:
  name: nodes-zone-label
spec:
  labels:
  - kvs:
      zone: az1
    nodes:
    - node1
    - node2
    - node5
  - kvs:
      zone: az2
    nodes:
    - node3
    - node4
    - node6
  - kvs:
      zone: reserved-arbitration
    nodes:
    - node0
```

#### 全新安装部署
在安装部署引导（bootstrap）阶段，*node-label-operator*就会运行，其根据上述配置的`nodelabel/nodes-zone-label`，为节点打上zone标签`zone.node-label.ccos.io`。

#### 扩容节点
集群扩容时，节点增加zone属性，并更新`nodelabel/nodes-zone-label`资源对象，为新增节点打zone标签。

> 集群扩容功能已支持

#### 现有环境升级
现有环境升级后，需要确保节点设置有zone标签。

具体的，登录运维页面，进入【运维】【系统】【区域】的主机列表页面为每个节点编辑`可用区`。
详见wiki[跨AZ高可用部署说明](https://wiki.cestc.cn/pages/viewpage.action?pageId=360074765) 。

### 将多个副本分散到不同的AZ
一些关键服务（例如envoy网关）虽然采用了多实例，但可能调度运行到同一个zone下面，若该zone整体故障，将导致网关长时间故障直到网关pod迁移到其它健在节点上、重新拉起。
为此，这些服务多个pod实例需要跨zone打散调度运行。

#### pod配置zone级别的pod反亲和
要**将副本分散到不同的AZ中**，可配置zone级别的pod反亲和，pod反亲和示例如下：
```yaml
...
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - webserver
          topologyKey: zone.node-label.ccos.io
...
```

> 注意，当副本数可能大于zone数量时，只能设置为非强制的反亲和性，否则会出现pod无法调度的情况

#### pod配置拓扑分布约束topologySpreadConstraints
要**将副本分散到不同的AZ中**，还可以配置pod拓扑分布约束topologySpreadConstraints，将pod在zone级别打散的示例如下：
```yaml
...
  topologySpreadConstraints:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - webserver
    maxSkew: 1
    topologyKey: zone.node-label.ccos.io
    whenUnsatisfiable: ScheduleAnyway
...
```

各配置项的说明详见文档[Pod Topology Spread Constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)。

相较通过pod反亲和实现zone级别的打散而言，`topologySpreadConstraints`是专门用于拓扑分布的，其性能开销更低，也不容易同pod的滚动更新/升级相冲突，
详见说明[Comparison with podAffinity and podAntiAffinity](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#comparison-with-podaffinity-podantiaffinity)。

#### 全局配置默认AZ级别的打散
不管是pod反亲和，还是`topologySpreadConstraints`，都需要为每个pod设置，比较麻烦。
为此，新增kube-scheduler调度策略（profile），实现集群层面全局默认AZ级别的打散。

目前，kube-scheduler-operator为kube-scheduler预置3个profile：
* `LowNodeUtilization`（默认）
* `HighNodeUtilization`
* `NoScoring`

新增`TopologySpread`策略profile，跨AZ打散。插件配置如下：
```yaml
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: zone.node-label.ccos.io
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
```

注意，集群层面的`Topology Spread Constraints`，当不满足zone打散时，`whenUnsatisfiable`调度策略配置为保守的`ScheduleAnyway`，也就是说这种跨zone的打散只能做到尽力而为，基于如下原因：
* 如果不满足zone打散时，调度策略为DoNotSchedule，将导致pod处于Pending，例如不兼容单AZ场景，导致co状态不多、扩容和升级时受影响、节点故障替换时受影响等
* 本来，结合Topology Spread Constraints的minDomains参数，可以做到只有多个zone时才执行跨az打散，这时whenUnsatisfiable可配置DoNotSchedule确保打散，但2+2+1，有仲裁节点时，仲裁节点是属于一个特殊的zone，且仲裁节点上有污点、默认不调度pod，仲裁zone的存在破坏minDomains参数对跨az打散的指导
* Topology Spread Constraints实际上支持nodeTaintsPolicy选项，将有污点的节点，例如仲裁节点，排除掉，但nodeTaintsPolicy默认关闭，从文档看，认为节点的污点是很易变的配置，人可以随便配置，关键节点状态异常、网络不对、资源耗尽都可能导致节点被打上污点，进而破坏跨AZ打散的调度计算，从而导致结果不准确

在kube-scheduler-operator支持`TopologySpread`策略基础上，CECSTACK V5产品默认配置为该策略，涉及如下两个方面的修改：
1. `scheduler.config/cluster`资源对象中`.spec.profile`默认设置为`TopologySpread`，在`cluster-config-operator`中修改：
   ```yaml
   apiVersion: config.ccos.io/v1
   kind: Scheduler
   metadata:
     name: cluster
   spec:
     profile: TopologySpread
   ```
   全新安装部署时，CECSTACK V5和ccos的版本，scheduler默认使用`TopologySpread`策略（其它产品码保持为空不变，默认LowNodeUtilization）。
2. 针对已有环境升级，`cluster-config-operator`中新增`defaultconfigcontroller`控制器，根据产品码更新默认配置。CECSTACK V5和ccos的版本时：
   * 若`scheduler.config/cluster`的`.spec.profile`为空，即没有人工修改过配置，则`defaultconfigcontroller`更新其为`TopologySpread`，更新默认配置的目的。
   * 若环境中已修改过调度策略，即`.spec.profile`字段不为空，则`defaultconfigcontroller`不更新这个字段，保持现场配置不变。

### 通过descheduler再平衡pod实现跨zone打散
前文中，全局配置的默认AZ级别打散调度策略为非强制的，当因为各种原因（例如zone级别的故障发生后pod全部迁移到同一个zone下），导致pod未能跨zone打散部署、无法应对下一次zone级别故障时，
需要驱逐pod，使其再平衡、跨zone打散部署。

另一方面，就扩容为跨AZ高可用部署的环境，已有的pod同样需要驱逐、重建，重新调度才能做到跨zone打散部署。

为此，针对跨AZ高可用场景，使能kube-descheduler，完成pod再平衡。

具体包括如下四个部分的修改：
1. `cluster-etcd-operator`，负责跨AZ高可用部署模式切换、节点标签管理等工作，当集群为跨AZ高可用部署（或者扩容而来）时，其通过创建`kdc`自定义资源对象，使能kube-descheduler-operator。
2. `copilot-operator`，其监听`kdc`资源对象，实际负责拉起kube-descheduler-operator，将descheduler默认策略调整为跨zone打散，并采用更加保守的descheduler执行周期，从1小时延长到24小时。
3. `cluster-kube-descheduler-operator`，其作为operator，负责管理`descheduler`，修改内容包括：
   * 默认将ccos-*和karmada-system命名空间纳入include，只对这些命名空间执行跨AZ打散部署驱逐策略（`RemovePodsViolatingTopologySpreadConstraint`）
   * 支持指定命名空间做pod跨zone再平衡，具体的，只需要为命名空间设置注解 `descheduler.ccos.io/include: "true"`
   * 增加对scheduler的profile（TopologySpread）和descheduler的profile（SoftTopologyAndDuplicates或TopologyAndDuplicates）的配套检查，只有scheduler采用跨zone打散部署测试时，descheduler才能相应的配置跨zone再平衡策略
4. `descheduler`，其具体负责pod再平衡，新增对topologySpread全局默认限制打散的支持（社区原生descheduler不支持默认的拓扑打散策略，其只认每个pod里配置的.spec.topologySpreadConstraints）


除上述内容外，针对测试中遇到的实际情况，调优kube-scheduler调度策略中，拓扑分布插件的权重，从默认的3调大到10，尽量将pod跨zone打散：
```yaml
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: default-scheduler
    plugins:
      score:
        disabled:
          - name: PodTopologySpread
        enabled:
          - name: PodTopologySpread
            weight: 10
    pluginConfig:
      ...
```
注意，在修改`PodTopologySpread`插件权重前，需要先`disable`关掉默认开启的插件，然后再`enable`打开该插件并修改权重，否则会报重复打开插件问题（详见[issues/91234](https://github.com/kubernetes/kubernetes/issues/91234)）。

### zone级别的亲和
存在依赖关系、有大量访问流量的一组pod，当需要将它们**亲和到同一AZ中**，可配置zone级别的pod亲和，示例如下：
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: application-server
...
spec:
  template:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - database
            topologyKey: zone.node-label.ccos.io
```

## 运行仲裁服务

### 仲裁节点初始化
安装部署和扩容时，仲裁节点作为管理角色（master）的节点加入Cell0管理集群，但应为其所属zone设置为特殊的仲裁区域`reserved-arbitration` 。
最终，仲裁节点带有`zone.node-label.ccos.io=reserved-arbitration`标签。

根据该标签，*cluster-etcd-operator* 自动为仲裁节点打上污点，**禁止非仲裁服务调度上去**：
```yaml
taints:
- effect: NoSchedule
  key: zone.node-label.ccos.io/reserved-arbitration
```

> 需要说明的：
> 1. 考虑到业务和实现内聚，由 *cluster-etcd-operator* 负责为仲裁节点打污点
> 2. 仲裁节点污点仅带*NoSchedule*的效果（Effect），同master节点污点保持一致，也就是说不会驱逐已调度的pod，尽量减少对环境带来的负面影响

#### 为仲裁服务设置污点容忍
为此，各仲裁服务**必须设置上述污点容忍**，否则无法运行在仲裁节点上。示例如下：
```yaml
  tolerations:
  - effect: NoSchedule
    key: zone.node-label.ccos.io/reserved-arbitration
    operator: Exists
```

#### 非关键组件不要调度到仲裁节点上
虽然仲裁节点设置了污点，pod默认不会调度上去（除非显式设置污点容忍），但有时控制器在管理工作负载副本数时，可能将仲裁节点计算进去，设置多个副本，但最终都无法成功调度。

为解决该问题，有如下两个措施。

##### 措施1：计算副本数时排除掉仲裁节点
仲裁节点是一种特殊的master节点，Operator在计算实例的副本数时，应排除掉仲裁节点，代码示例如下：
```golang
	nodeLister := kubeInformersForNamespaces.InformersFor("").Core().V1().Nodes().Lister()
	// 使用 nodeCountFunc 计算副本数时，排除掉了仲裁节点
	nodeCountFunc := func(nodeSelector map[string]string) (*int32, error) {
		// CCOS: exclude arbitration node
		r, err := labels.NewRequirement("zone.node-label.ccos.io", selection.NotEquals, []string{"reserved-arbitration"})
		if err != nil {
			return nil, err
		}

		ls := labels.SelectorFromSet(nodeSelector).Add(*r)
		nodes, err := nodeLister.List(ls)
		if err != nil {
			return nil, err
		}
		replicas := int32(len(nodes))
		return &replicas, nil
	}
```

##### 措施2：设置仲裁节点反亲和
要进一步确保副本不会调度到仲裁节点，可为工作负载设置强制的仲裁节点反亲和：
```yaml
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: zone.node-label.ccos.io
                  operator: NotIn
                  values:
                    - reserved-arbitration
```

上述措施特别适用于由HPA管理副本数、在压力下可能拉起很多副本的情况。

### 仲裁节点软硬件要求
仲裁节点软硬件规格、机型同Cell0管控节点保持一致，并会加入Cell0管理集群。

具体的：
1. **minio对仲裁节点要求**
  > 同管控节点机型相同，有数据盘
2. **etcd对仲裁节点要求**
  > 1. ssd盘，用于给etcd的存储空间预留15GB+
  > 2. CPU，用于etcd使用的资源2C+
  > 3. 内存，用于etcd使用的资源8GB+
  > 4. 网络，同AZ中各管控节点采用1000Mbps网络，rtt时延不超过50ms
  > 参见：
  > * https://etcd.io/docs/v3.5/op-guide/hardware/
  > * https://etcd.io/docs/v3.5/tuning/
3. **ovsdb对仲裁节点要求**
  > 同管控节点机型相同
4. **zookeeper对仲裁节点要求**
  > 同管控节点机型相同

### 网络通信
仲裁节点会加入Cell0管理集群，容器网络和service访问上对各运行有仲裁服务的有状态服务没区别，各仲裁服务无感。

## 故障切换和高可用详细设计
仲裁节点会作为Cell0管理集群一员、加入集群，为此需要运行仲裁服务的有状态服务在安装部署、服务发现、网络通信方面不需要额外的工作。

但需要重点说明的，仲裁节点只能调度运行仲裁服务，其上携带如下污点：
```yaml
taints:
- effect: NoSchedule
  key: zone.node-label.ccos.io/reserved-arbitration
```
各仲裁服务**必须设置上述污点容忍**，否则无法运行在仲裁节点上。


### etcd
涉及如下内容：
* 仲裁节点上etcd如何部署
* 管理集群cluster-etcd-operator和etcd如何调整：member管理；证书轮转；co状态表达；更新。

#### 高可用指标

| 场景                                                                                             | 高可用指标                                                                                                       |
|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| 适用于同城有数个数据中心，数据中心的时延时 RTT < 50ms，<br>其中 etcd数据存储于系统盘(磁盘io < 10ms)，系统盘空间 > 15G，etcd资源占用要求 2C 8G | RPO=0，当前跨AZ高可用方案，数据不会丢失。<br>RTO < 10s，当其中一个AZ发生数据中心故障或者设备故障，etcd可能会进行leader 的切换，当需要进行切换时，10s 内etcd集群可重新恢复使用 |

#### 高可用部署架构
* cell0跨AZ部署，cell0上总共会启动4个管控节点，其中2个管控运行在AZ1，另外2个管控运行在AZ2
* AZ1上的管控节点会分别运行2个etcd服务(etcd0, etcd1)；AZ2上2个管控节点也会运行2个etcd服务(etcd2 etcd3)，独立于AZ1和AZ2之外的仲裁节点上运行的etcd仲裁服务(etcd4)
* etcd0-4 共5个etcd节点共同组建出etcd集群（组建的集群会选择一个节点作为leader：例如下图 etcd leader 是 etcd2服务）

![img.png](images/etcd跨AZ高可用部署架构.png)

##### 容灾场景1：etcd leader所在节点的AZ发生故障
etcd leader 是管控3上的etcd2节点，此时AZ2发生集群故障，导致管控3和管控4上的控制面组件不能正常提供服务。

| 故障恢复性能指标 | 值和说明                                      |
|----------|-------------------------------------------|
| RPO      | 0，数据不会丢失                                  |
| RTO      | ~7秒，由于此时会发生etcd leader主从切换(etcd2 → etcd0) |

![img.png](images/etcd跨AZ高可用部署-leader切换.png)

##### 容灾场景2：非etcd leader所在节点的AZ发生故障
etcd leader 是管控1上的etcd0节点，此时AZ2发生集群故障，导致管控3和管控4上的控制面组件不能正常提供服务。

| 故障恢复性能指标 | 值和说明            |
|----------|-----------------|
| RPO      | 0，数据不会丢失        |
| RTO      | 0，此时etcd不发生主从切换 |

![img.png](images/etcd跨AZ高可用部署-故障时etcd不需要切主.png)

##### 容灾场景3：AZ间网络隔离
AZ间网络隔离，但各AZ同仲裁节点的网络正常，在仲裁服务的介入下，最终都能选出leader来。

> 注意，etcd的leader跑在仲裁节点上时，etcd集群对外照常服务，kube-apiserver访问etcd不受影响。

具体的，分三种情况
1. 网络隔离前，leader是仲裁节点，由于etcd集群leader同follower通信不受影响，故障时不涉及leader切换（RPO为0，RTO为0）
2. 网络隔离前，leader在AZ1，隔离后AZ2中的follower长时间收不到leader的心跳、发起选举，进入短暂的震荡状态，最终只有仲裁节点发起的选举才会收到大家的认可，进而仲裁节点切为主，etcd集群恢复（RPO为0，RTO不长于10秒）
3. 同上述第2种情况，最终稳定在仲裁节点作为主

> 短暂震荡状态的进一步说明：
> 当AZ间网络隔离时，只要不是仲裁节点做leader，都会导致非leader的AZ中节点发起leader选举（毕竟这些follower接收不到来自另一个AZ的leader的心跳），从而产生选举震荡。
> 不过，etcd采用的raft协议中各候选人随机退避、发起选举，只要仲裁节点发起选举，便会收到多数选票，进而整个集群稳定在仲裁节点做leader的状态。

#### 详细设计
集群的高可用，自动恢复，需要保证etcd集群运行所需要的法定运行个数(大于法定人数一半的成员可以正常工作)，即需要一个独立运行在AZ1和AZ2之外的仲裁服务。

##### 安装etcd仲裁服务
CECSTACK V5采用标准部署，部署完成后，根据需要扩容仲裁节点，在仲裁节点上拉起etcd仲裁服务，具体的：
1. 当单AZ扩容为两AZ，或者直接规划两AZ时，向管理集群中添加仲裁节点（一种特殊的管控master节点）
2. *cluster-etcd-operator*处理*etcd*调度，为仲裁节点打上*etcd*标签（借助*node-label-operator*提供的能力）
3. *cluster-etcd-operator*在仲裁节点上拉起etcd仲裁服务

##### 管控节点etcd标签管理
关键的有状态服务mysql、minio，以及kafka、clickhouse和dts的zookeeper均亲和etcd标签。
只需要etcd做好拓扑计算和调度，上述服务无需单独计算高可用调度问题，即可做到高可用部署。

etcd-operator负责etcd的安装部署，其使用*staticpod*库安装etcd。
**但该库存在局限**，硬编码亲和`node-role.kubernetes.io/master=`标签，导致每个master/管控节点上都会拉起etcd。
未来考虑集群大规模，势必管控扩容，例如从现在的3节点扩容为5节点、10节点，甚至更多，相应的拉起10多个etcd节点不可取。

为此，当前改造了etcd-operator使用的*staticpod*库，通过配置控制etcd拉起模式，支持：
* 最多拉起3个etcd（默认）
* 每个master节点拉起etcd

上述方式存在的问题：
* 对*staticpod*库侵入性强
* 不灵活，不支持更复杂的拓扑计算，例如跨AZ高可用、AZ扩容
* 不好推广，当前scheduler-operator、kcm-operator和apiserver-operator都是使用*staticpod*拉起operand实例，同样的管控扩容的问题需要解决

综上，etcd-operator做如下改造：
1. 新增etcd节点标签控制器`nodelabelcontroller`，负责计算etcd标签，并创建*nodelabel*资源，让*node-label-operator*打上etcd标签
   > 自动应对单AZ扩两AZ，及两AZ高可用部署（带仲裁节点）的场景。
   > 所有的复杂运算，均聚拢到`nodelabelcontroller`实现逻辑中。
2. 优化*staticpod*，支持传入节点标签，不再硬编码亲和`node-role.kubernetes.io/master=`标签
   > 具体的，传入etcd标签，只在对应节点上拉起etcd。
   > 当不传节点标签时，默认亲和master，保持兼容性。
3. 考虑到etcd的重要性和健壮性需要，`nodelabelcontroller`计算好etcd调度节点、创建*nodelabel*资源后，可主动为节点打上etcd标签，而不用等到*node-label-operator*处理，避免*node-label-operator*的软件bug或者其它循环依赖导致的集群部署故障

##### etcd节点标签控制器
etcd-operator中新增etcd节点标签控制器`NodeLabelController`，负责管理etcd节点标签。

etcd集群有三种工作模式：
* ***traditional***，**传统模式**，对应于CCOS安装部署V1，etcd-operator拉起etcd集群的方式完全兼容现有逻辑，即会在每个master节点上拉起etcd。
* ***standard***，**标准模式**，对应于CCOS安装部署V2，当master节点大于等于5个时etcd集群采用5节点规模，当master节点小于5个大于等于3个时etcd集群采用3节点规模。
* ***multiZoneHA***，**跨AZ高可用模式**，在 **标准模式** 模式基础上，etcd采用跨zone部署，应对zone级别的故障。

首先，`NodeLabelController`根据`kube-system`命名空间下`cm/cluster-config-v1`安装部署配置信息，判断集群为*UPI*或*IPI*模式：
* 当`platform`字段为`baremetal`，且配置有`hosts`时，表明为*IPI*模式（对应于CCOS支持的安装部署V2），ETCD集群工作在 **标准模式** 或 **跨AZ高可用模式**
* 否则，为*UPI*模式，etcd集群工作在 **传统模式** 模式

**传统模式**下，为*staticpod*传入master节点标签，即在每个master节点上拉起etcd，同时`NodeLabelController`监听mater节点，由其为master节点无条件打上etcd标签。

`NodeLabelController`根据集群节点的zone数量和zone中master节点数量，进一步判断为 **标准模式** 或 **跨AZ高可用模式** 。

如果zone数量小于3，或者每个zone中的master数量不够，则etcd工作在**标准模式**。
**标准模式**中，`NodeLabelController`会：
* 获取master节点列表
* 如果master数量大于等于5，则选取其中5个节点打上etcd标签
* 如果master数量大于等于3、小于5，则选取其中3个节点打上etcd标签
* 如果master数量小于3，则在每个master节点上打上etcd标签，同传统模式保持一致

如果zone数量大于等于3（当存在仲裁节点时，包含仲裁节点在的zone），且zone中的mater数量足够组成`2+2+1`或`1+1+1`跨AZ高可用部署拓扑时，etcd工作在**跨AZ高可用模式**。
`NodeLabelController`会从3个zone中比较、选取master节点，为其打上etcd标签，确保etcd跨AZ高可用部署。需要说明的：
* 在能够凑齐`2+2+1`拓扑时，其中1优先从仲裁zone中选取仲裁节点打上etcd标签，剩下两个区域，优选master节点多的区域，且每个区域各选取两个master节点打上etcd标签。
* 在不能凑齐`2+2+1`拓扑时，采用`1+1+1`拓扑，仍然优先从仲裁区域中选取仲裁节点打上etcd标签，剩下两个区域，按照区域中master节点数量排序，优选master节点多的区域，每个区域各选取一个master节点打上etcd标签。

最终，由*staticpod*控制器监听到带有etcd标签的master节点，在这些节点上安装etcd pod，部署etcd集群。

### mysql
mysql采用MGR单主模式（[Single-Primary Mode](https://dev.mysql.com/doc/refman/8.0/en/group-replication-single-primary-mode.html)），
通过Paxos实现一致性和选主。
同时通过service指向mysql主，向集群内其它容器（容器网络）提供数据库服务。
若在仲裁节点上拉起mysql，且一旦仲裁节点上mysql切为主，将导致集群内其它容器（通过容器网络）无法访问仲裁节点上的mysql主。

@贾冉绯

设计文档链接[https://wiki.cestc.cn/pages/viewpage.action?pageId=345611436](https://wiki.cestc.cn/pages/viewpage.action?pageId=345611436) 。

### minio
@连建永

经评审，选择**仲裁服务方案**，详见链接 [0016-MinIO跨AZ高可靠的开发设计文档](https://wiki.cestc.cn/pages/viewpage.action?pageId=347027957) 。

附[0017-MinIO扩容方案的调研](https://wiki.cestc.cn/pages/viewpage.action?pageId=349212546)

### ovsdb
@杨志祥

设计文档链接[【方案设计】ceastack V5 双AZ高可用设计](https://wiki.cestc.cn/pages/viewpage.action?pageId=345614310) 。

### redis
redis采用哨兵模式，其Operator自动完成故障切换。要支持跨两个AZ的高可用部署场景，需要增加处理逻辑即可。

#### 当前拉新从条件

+ Redis 容器长时间处于 pending 状态
+ Redis 容器重启过多次并仍然处于不健康状态，可能硬件损坏导致
+ Redis 节点一定时间内 NotReady

由于 sentinel 三个节点分别亲和到三个 master，如果单个 master 节点故障，sentinel 剩下两个节点能继续提供服务，故未做拉新从处理，故障节点恢复后，sentinel 自动恢复。

#### 跨 AZ 高可用

增加亲和性设置，使主备节点部署到不同 zone，单 AZ 故障可通过拉新从自动创建新的 Redis 容器。

同时，需要对 sentinel 也做拉新从处理，使能够在新的节点自动创建 sentinel 来组建管理面 redis 集群。


### kafka
kafka分为2部分

一.Kafka

1.1 Kafka的高可用使用kafka多副本能力，亲和az，使kafka broker孵化在不同的az上

1.2 数据副本可以使用kafka的机架配置参数：broker.rack 将亲和某个az的broker节点的broker.rack配置成azN,会使kafka数据副本相对均匀的分配到2个az的broker上，确保topic分区副本会落在不同az的broker上。

二.ZooKeeper

组件Kafka将共用一个ZooKeeper。Zookeeper三副本，分别亲和到AZ1, AZ2和仲裁节点。zk1 亲和 AZ1, zk2亲和AZ2, zk3手工部署在仲裁节点。

三.高可用说明

3.1 kafka高可用

假如当前kafka的topic分区副本leader在AZ1

AZ1 宕机后，kafka的topic 分区副本 leader会自动切换到AZ2的分区副本中，无数据丢失。operator 在AZ1 重新拉起新的sts, 故障恢复后pod起来从AZ2上的broker 分区副本中同步数据。

AZ2 宕机，集群无感知。operator 在AZ2 重新拉起新的sts, 故障恢复后pod起来从zk1/zk3中同步数据。

3.2 zookeeper高可用

假如当前的Leader为zk1。

AZ1 宕机后，重新选举Leader, zk2/zk3成为新的Leader，无数据丢失。operator 在AZ1 重新拉起新的sts, 故障恢复后pod起来从zk2/zk3中同步数据。

AZ2 宕机，集群无感知。operator 在AZ2 重新拉起新的sts, 故障恢复后pod起来从zk1/zk3中同步数据。

仲裁节点宕机，无感知。在仲裁节点恢复后(清理data目录)重新启动zk进程。

3.3 k8s集群内的zk pod使用host ip便于仲裁节点上的zk3访问

3.4 前置条件

3.4.1 端口需要重新规划因为使用host ip，容易有端口冲突

3.4.2 仲裁节点的ip，最好在集群boot的时候就能确定并且以后不再发生变化

3.4.3 仲裁节点最好提供docker环境
@李康宁

### clickhouse
@胡颖新 @张敏 @李林

设计文档链接[ccos clickhouse 跨两个AZ管控集群高可用方案调研](https://wiki.cestc.cn/pages/viewpage.action?pageId=345617273) 。

### dts
dts分为3部分

一. 无状态服务

无状态服务亲和master 标签，当发生故障时候，利用k8s的能力将pod漂移到当前可用的AZ上。

二.Kafka

2.1 Kafka的高可用依托组件Kafka的能力

2.2 数据副本数和offset副本数，已与Kafka沟通，多副本的数据会反亲和到不同的AZ上。offset亦如此。

三.ZooKeeper

所有的组件dts共用一个ZooKeeper。Zookeeper三副本，分别亲和到AZ1, AZ2和仲裁节点。zk1 亲和 AZ1, zk2亲和AZ2, zk3手工部署在仲裁节点。

3.1 高可用说明

假如当前的Leader为zk1。

AZ1 宕机后，重新选举Leader, zk2/zk3成为新的Leader，无数据丢失。operator 在AZ1 重新拉起新的sts, 故障恢复后pod起来从zk2/zk3中同步数据。

AZ2 宕机，集群无感知。operator 在AZ2 重新拉起新的sts, 故障恢复后pod起来从zk1/zk3中同步数据。

仲裁节点宕机，无感知。在仲裁节点恢复后(清理data目录)重新启动zk进程。

3.2 k8s集群内的zk pod使用host ip便于仲裁节点上的zk3访问

3.3 前置条件

3.3.1 端口需要重新规划因为使用host ip，容易有端口冲突

3.3.2 仲裁节点的ip，最好在集群boot的时候就能确定并且以后不再发生变化

3.3.3 仲裁节点最好提供docker环境

@杨辉

## 部署和扩容
### 两AZ高可用部署
全新规划的两AZ高可用环境，部署方案如下：

|     | AZ1    | AZ2    | 仲裁节点 | 说明                            |
|-----|--------|--------|------|-------------------------------|
| 标配  | m1, m2 | m3, m4 | a1   | 5台管控（含仲裁节点），实现两AZ高可用标准部署      |
| 低配* | m1     | m2     | a1   | 根据需要，也可3台管控（含仲裁节点），实现两AZ高可用部署 |

> 注意： 低配* 暂不支持，在这种场景下除开仲裁节点仅剩两个master，目前CECSTACK V5不支持两master部署。
> 验证过程详见[wiki记录](https://wiki.cestc.cn/pages/viewpage.action?pageId=355969214#id-%E8%81%94%E8%B0%83-%E6%9A%82%E4%B8%8D%E6%94%AF%E6%8C%81%E4%B8%A4master)

### Region内AZ扩容
针对客户已有一个AZ，现在希望扩容到两AZ高可用时，Cell0管理集群的管控节点扩容方案如下：

| 阶段            | AZ1        | AZ2     | 仲裁节点 | 说明                                                                                                    |
|---------------|------------|---------|------|-------------------------------------------------------------------------------------------------------|
| 初态            | m1, m2, m3 |         |      | 初始状态，现有一个AZ中，有3个管控节点                                                                                  |
| 替换管控节点        | m1, m2     | m3*     |      | 从新AZ2中，选取一个节点m3*，替换管控节点m3，**m3*的zone属性为AZ2**                                                          |
| 新增仲裁节点        | m1, m2     | m3*     | a1   | 仲裁节点a1，**a1的zone属性为仲裁区域**<br> *(etcd首先调整拓扑为1+1+1，将AZ1中m1或m2上的etcd迁移到a1节点上，实现跨AZ高可用部署)*                |
| minio跨AZ高可用改造 | m1, m2     | m3*     | a1   | minio将m2上的副本迁移到a1仲裁节点上<br> *(至此minio跨AZ高可用部署)*                                                        |
| 新增管控节点        | m1, m2     | m3*, m4 | a1   | 新AZ中，增加一个管控节点m4<br> *(etcd会再次调整集群拓扑为2+2+1，满足跨AZ高可用部署)*                                                |
| 管控节点负载再平衡     | m1, m2     | m3*, m4 | a1   | 为仲裁节点a1打上污点，驱逐上面的非仲裁服务，并让各管控节点负载再平衡<br>*（etcd-operator完成etcd跨AZ高可用部署后，使能descheduler实现pod再平衡、跨zone打散）* |
| 扩容完成          | m1, m2     | m3*, m4 | a1   | 最终完成两AZ高可用扩容                                                                                          |

> **注意:**<br>
> 已有Cell0管理集群，**应先升级到支持跨两个AZ高可用部署的版本，再行AZ扩容。**

## 友商参考对比
### 华为云
[华为云Stack 8.2.0 灾备解决方案技术白皮书](https://e.huawei.com/cn/material/enterprise/f05651b5eee6453a8fe23a42d548b664) ：

![img.png](images/华为云管理组件跨AZ高可用.png)

#### CECSTACK V5和华为云跨两个AZ高可用方案对比
![img.png](images/2az-cecstackv5-vs-huaweicloud.png)

详见胶片[华为云跨AZ高可用分析对比](resources/华为云跨AZ高可用分析对比.pptx) 。
